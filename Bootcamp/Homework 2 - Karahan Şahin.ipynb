{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b841b551",
   "metadata": {},
   "source": [
    "# Homework 2 - Entropy, Cross-Entropy, KL Divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c82e3014",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2009528",
   "metadata": {},
   "source": [
    "## Information Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7febc23",
   "metadata": {},
   "source": [
    "Bit information is used with 1s and 0s. This bit information is used for reduction of the uncertainty of the information with the factor of 2. It sounds complicated. So how elaborate this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f726c6b",
   "metadata": {},
   "source": [
    "Think of a person you have never met. What information you need to know. Maybe you want to their gender or they are old or young. Assuming this information comes binary and at the equal probability. If we want to encode this information with 0 and 1's, \n",
    "\n",
    "<img src=\"https://image.shutterstock.com/image-vector/male-female-symbol-icon-vector-260nw-1190374711.jpg\" style=\"\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0cba2e",
   "metadata": {},
   "source": [
    "the bit size is the total number of random variables factored by two:\n",
    "   \n",
    "* Bit size: log(2) = 1 bit\n",
    "    \n",
    "Meaning that for male, we encode **male** as '0' while encode **women** as '1'. But if we ask what is the informativeness of gender to you? Then we need **entropy**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c0cb20",
   "metadata": {},
   "source": [
    "## Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937aee81",
   "metadata": {},
   "source": [
    "Entropy is the average information you receive from a distribution of given occurrances. Entropy is calculated by a negative sum of the probability of event multiplied by the $\\log_2$ of probability, meaning that with the true possibility of all eventsâ€™ occurrence how much information can be received. Information is larger when the probability is smaller because $\\log_2$\n",
    "\n",
    "$ H(p) = -\\sum_{i} p_i * \\log_2(p_i)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d1c4200",
   "metadata": {},
   "outputs": [],
   "source": [
    "probability_of_gender =[0.5, 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7c62125",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_entropy(prob):\n",
    "    return -np.array([i*np.log2(i) for i in prob]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a65cdfc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_entropy(probability_of_gender)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df6e8f9",
   "metadata": {},
   "source": [
    "But if we say that the distributions are not equal and it is more likely for you encounter a female than a male?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e7a3838",
   "metadata": {},
   "outputs": [],
   "source": [
    "probability_of_gender =[0.75, 0.25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81a79394",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8112781244591328"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_entropy(probability_of_gender)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593970f2",
   "metadata": {},
   "source": [
    "Then we see that bit-wise information reduces since the your chance of encountering a female is more likely, your uncertainty of the person's gender gets smaller.\n",
    "\n",
    "But what do you think the gender of that person? If you want to find your expectation meets with the reality, you need **cross-entropy**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7198166f",
   "metadata": {},
   "source": [
    "## Cross Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3632cb",
   "metadata": {},
   "source": [
    "Cross entropy on the other hand is **the average message length** from a distribution of given probabilities.\n",
    "Cross entropy is calculated by a negative sum of actual probability of events multiplied by the predicted informativeness $log_2(p(event)$ of events. This gives the expected information length occured via your prediction.\n",
    "\n",
    "$ H(p, q) = -\\sum_{i} p_i * \\log_2(q_i)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8781cc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cross_entropy(true_prob, predict_prob):\n",
    "    return -np.array([t*np.log2(p) for t,p in zip(true_prob, predict_prob)]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1bd408d",
   "metadata": {},
   "outputs": [],
   "source": [
    "probability_of_gender = [0.5, 0.5]\n",
    "prediction_of_gender = [0.75, 0.25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eab57984",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.207518749639422"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_cross_entropy(probability_of_gender, prediction_of_gender)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8b66fc",
   "metadata": {},
   "source": [
    "There we can say that our expectations haven't met with the true probability of gender. Your uncertainty of  The difference of the informativeness of the gender and the informativeness of the expectation of gender gives us the KL Divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a320f3",
   "metadata": {},
   "source": [
    "## KL Divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d62d112",
   "metadata": {},
   "source": [
    "KL divergence can be calculated as the negative sum of probability of each event in P multiplied by the log of the probability of the event in Q over the probability of the event in P. \n",
    "\n",
    "$D_{KL}(P || Q) = H(p, q) - H(p)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad4023bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_kl_divergance(true_prob, predict_prob):\n",
    "    return calculate_cross_entropy(true_prob, predict_prob) - calculate_entropy(true_prob) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40abc109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20751874963942196"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_kl_divergance(probability_of_gender, prediction_of_gender)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa13c52",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e836703",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
